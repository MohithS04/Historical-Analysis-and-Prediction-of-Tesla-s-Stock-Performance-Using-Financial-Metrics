---
title: Historical Analysis and Prediction of Tesla's Stock Performance Using Financial
  Metrics
subtitle: 'MATH 40028/50028: Statistical Learning'
date: "October 22, 2024"
output:
  pdf_document: default
  html_document:
    df_print: paged
fontfamily: mathpazo
fontsize: 11pt
header-includes: \linespread{1.05}
urlcolor: blue
---

__ACADEMIC INTEGRITY: Every student should complete the project by their own. A project report having high degree of similarity with work by any other student, or with any other document (e.g., found online) is considered plagiarism, and will not be accepted. The minimal consequence is that the student will receive the project score of 0, and the best possible overall course grade will be D. Additional consequences are described at http://www.kent.edu/policyreg/administrative-policy-regarding-student-cheating-and-plagiarism and will be strictly enforced.__

## Instruction

__Goal:__ The goal of the final project is to apply the statistical learning methods discussed in this course to perform predictive analysis of real-life data. You will need to identify prediction problem(s), carry out necessary exploratory data analysis, perform predictive analysis using statistical learning methods, assess the performance, and communicate the results in a report. 

__Report:__ Use this Rmd file as a template. Edit the file by adding your project title in the YAML, and including necessary information in the four sections: (1) Introduction, (2) Statistical learning strategies and methods, (3) Predictive analysis and results, and (4) Conclusion. 

__Submission:__ Please submit your project report as a PDF file (8-10 pages, flexible) to Canvas by __11:59 p.m. on December 8, 2024__. The PDF file should be generated by “knitting” the Rmd file. You may choose to first generate an HTML file (by changing the output format in the YAML to `output: html_document`) and then convert it to PDF. Word documents, however, cannot be used as an intermediate file (and of course, the submitted file). __20 points will be deducted if the submitted files are in wrong format.__ 

__Grade:__ The project will be graded based on your ability to (1) recognize and define prediction problems, (2) identify potentially useful statistical learning methods, (3) perform the predictive analysis and assess the performance, (4) document the analysis procedure (with R code) and clearly present the results, and (5) draw valid conclusions supported by the analysis.

__Datasets:__ You may consider (but are not restricted) to use the following packages/datasets. 

* [`ISLR2`](https://cran.rstudio.com/web/packages/ISLR2/ISLR2.pdf): datasets used in the _Introduction to Statistical Learning_ textbook
* [`dslabs`](https://cran.r-project.org/web/packages/dslabs/dslabs.pdf)
* [UCI Machine Learning Repository](https://archive.ics.uci.edu/)

\pagebreak

## Introduction [15 points]

* Describe the dataset. What is the dataset about? 

Below is presented the history of Tesla, Inc.'s stock price, while including thorough daily financial metrics usually considered crucial in any analysis and consequent prediction of trends at the stock market. Examples of such variables are the opening, closing, high, and low prices every trading day of the respective securities, which would show the movement and volatility of the stock's price within a session. It also involves trading volume, which indicates the extent of market activity and investor interest in a particular stock, and adjusted closing price, which accounts for corporate actions such as stock splits and dividends. Percentage changes in prices and average volumes over derived time horizons add value to the dataset, thereby enriching trend analysis, momentum detection, and volatility assessment. This dataset, ranging over a few years, reflects the stock performance of Tesla during its different phases of the corporate lifecycle, offering an all-rounded view of how the firm has been through the vagaries of new product launches, market expansions, and other economic events. The engineered features that add depth to it include moving averages, exponential moving averages, and volatility measures. The temporal richness in this dataset makes it a very valuable resource in the study of predictive modeling, market efficiency, and investor behavior in equity markets, while supporting the evaluation of trading strategies and financial patterns.The Dataset contains 3472 observations and 9 variables, The link for dataset is : https://www.kaggle.com/datasets/guillemservera/tsla-stock-data/data

```{r}
library(tidyverse)
library(dplyr)
library(tidyr)
library(caret)
library(cluster)
library(rlang)
library(naniar)
library(corrplot)
library(tidymodels)
library(MASS)
library(zoo)
library(TTR)
library(quantmod)
```
```{r}
tesla_data <- read.csv("tesla_ev.csv")
str(tesla_data)
```
```{r}
null_value <- is.na(tesla_data)
colSums(null_value)
```
```{r}
tesla_data$change_percent[is.na(tesla_data$change_percent)] <- mean(tesla_data$change_percent, na.rm = TRUE)
tesla_data$avg_vol_20d[is.na(tesla_data$avg_vol_20d)] <- mean(tesla_data$avg_vol_20d, na.rm = TRUE)
```

For this reason, imputation for missing values for change_percent and avg_vol_20d will be done with the mean, since this is an easy and good way to keep the dataset size intact without losing the tendency of the data. Therefore, when these missing values are replaced with the mean, then the distribution of these two variables as a whole would remain consistent, and none of the rows are removed, meaning that there will be completeness of data to analyze. Although mean imputation does not take into account relationships between variables and variability, being a practical choice for continuous numerical data, it is computationally very efficient as a first step for exploratory data analysis.

```{r}
null_value_updated <- is.na(tesla_data)
colSums(null_value_updated)
```
```{r}
head(tesla_data)
```
```{r}
#convert date column to date type
tesla_data$date <- ymd(tesla_data$date)
```

* Describe the statistical learning approaches and other strategies for feature engineering (transformation, selection, etc.).

```{r}
# Convert to a binary outcome (e.g., 1 for positive % change, 0 otherwise)
tesla_data$binary_target <- ifelse(tesla_data$change_percent > 0, 1, 0)

# Feature Engineering: Moving Averages and Exponential Moving Averages
tesla_data <- tesla_data %>%
  mutate(moving_avg_5d = rollapply(close, width = 5, FUN = mean, fill = NA, align = "right"),
         moving_avg_10d = rollapply(close, width = 10, FUN = mean, fill = NA, align = "right"),
         exp_moving_avg_5d = EMA(close, n = 5),
         exp_moving_avg_10d = EMA(close, n = 10),
         volatility_10d = rollapply(close, width = 10, FUN = sd, fill = NA, align = "right"),
         volume_change = volume / lag(volume) - 1,
         volume_avg_5d = rollapply(volume, width = 5, FUN = mean, fill = NA, align = "right"))

# Drop NA rows created by moving averages (first few rows will be NA)
tesla_data <- na.omit(tesla_data)
```

The work has been designed to analyze and forecast the stock behavior of Tesla using statistical learning approaches with extensive support from feature engineering strategies. One of the important statistical learning methods to be used is logistic regression, which is able to present comprehensible models and suits binary classification problems, for instance, whether the stock percent change will be positive or not. Extensive feature engineering was done to enrich the dataset by creating new variables for temporal and volatility patterns in the stock price. To smooth out the noise at the short term and thereby emphasize the trend, the 5-day and 10-day moving averages and exponential moving averages were generated. The 10-day rolling standard deviation from closing prices was computed to provide the volatility of the market, thereby giving a hint about the stability of the stock over time.

Other features engineered included volume-related features, such as percentage changes in volume and 5-day average volumes, in order to account for trading activity dynamics. Standardization of the numerical variables was one of the data transformations done to ensure consistency and improve performance. Besides, one of the major transformation steps was the creation of a binary target that changed the percentage change data into a categorical format for classification. Feature selection was implicitly done by focusing on those variables that demonstrated significant predictive value, including close, volume, and high. These together ensured that the data set was well-prepared for statistical learning, where meaningful patterns and relationships in the data had been modelled.
```{r}
str(tesla_data)
```
This dataset contains 3,463 entries across 17 variables for these financial metrics and trends beyond July 13, 2010. The key variables capture the daily trading: starting price, high and lowest, close, volume amount, adjusted_close prices, and change_percent to record positive or negative price changes from one day to the other. For trend analysis: moving averages, both moving_avg_5d and moving_avg_10d, and exponential kinds, exp_moving_avg_5d and exp_moving_avg_10d-termed, together with a measure of 10-day volatility, volatility_10d. The variables avg_vol_20d, volume_change, and volume_avg_5d describe the volume trends. The binary_target variable shows the direction of the movement of the market, which may be very useful in predictive modeling. The data was populated with dummy values for demonstration; 9 observations were excluded owing to missing entries to ensure completeness and integrity. This strong dataset forms a very good basis for financial trend analysis and decision-making.

* If possible, comment on the target population, sampling strategies, potential bias, etc. 

Target Population:
The target variable is the percentage change in the closing price of the Tesla stock, categorized positively or negatively, within the context of the target population. This binary classification allows for easier predictive modeling to the target population of financial analysts, data scientists, traders, and investors by enabling them to forecast directional movements in Tesla's stock price. It forms the basis for detecting situations of the market as bullish or bearish and helping make appropriate trading decisions or model robust financial forecasting.

Sampling Strategies:
This dataset is an instance of a multi-year timescale involving daily stock prices and the financials of Tesla, including temporal features such as historical records for moving averages, exponential moving averages, and measures of volatility. Features range from rolling standard deviations down to changes in volume over differently sized time windows, providing this robust temporal sampling framework. Sampling strategies could involve a chronological split of data into training, validation, and test sets-say, using the most recent year for testing while training takes place on earlier data. In the case of classification, creating a balanced dataset where, say, the number of positive and negative percentage changes in closing prices are equal, will ultimately improve model reliability.

Potential Bias:
There could be biases in the dataset since it is based only on Tesla and thus may not generalize to other stocks or market conditions. It will also pick up market anomalies specific to Tesla's life cycle-for instance, the rapid growth of the company, its product launches, or times of high investor sentiment. The data will have temporal biases if the conditions during this selected time were abnormal compared to the market performance over a long period, including economic booms or depressions. Moreover, specific engineered characteristics such as moving average and exponentially weighted moving averages may potentially amplify certain trend factors in their data instead of sharp, unexpected events that rock the market to its foundations, hence reflecting bias or prejudice in reporting actual markets.

* Identify and define prediction problem(s).

Several different predictions could be made from this dataset of Tesla Stock Price: classify the binary problem to predict if the stock price will close higher or lower than it has from the previous day to support a directional trading strategy, or for regression tasks, predict the exact percentage change in stock price to help estimate potential risks and optimize a trading strategy. The major applications include time series forecasting, which would predict the closing price of the future for medium- and long-term investment decisions. Other related prediction problems are volatility prediction that considers the market risk, volume for the estimation of liquidity and detection of atypical activities, and identification of moving average crossovers signaling the bullish or bearish condition of the market. It can also be used in event impact analysis to predict the result of major events on stock price or volume and provide indications of when to undertake pre-event trades. These problems utilize the dataset's temporal and financial metrics to solve real-world problems in financial forecasting.


* Discuss how to split the data into training and test sets among other plans for the use of data.
```{r}
# Split the data into training and test sets
set.seed(123)
train_index <- createDataPartition(tesla_data$binary_target, p = 0.8, list = FALSE)
train_data <- tesla_data[train_index, ]
test_data <- tesla_data[-train_index, ]
```

This splitting is good to make the Tesla stock price dataset valid for some predictive capability, ensuring the generalization of any model without over-fitting. Normally, 80% of data could go for training and 20% for testing, since it is important that the model learns from historical patterns and gets validated on unseen data. Temporal order has to be preserved in order to avoid leakage, using older data for training and recent data for testing. Other strategies include cross-validation on the training set for tuning hyperparameters and evaluating model robustness. Feature scaling and transformation-numeric variable normalization and temporal feature engineering, for example-should be consistently applied to both subsets. Implementation of the above practices would therefore provide effective utilization of the dataset in the building and validation of reliable predictive models.

## Statistical learning strategies and methods [35 points]

* Perform exploratory data analysis using the training set.
```{r}
# Line plot for Price trends over time
ggplot(train_data, aes(x = date)) +
  geom_line(aes(y = open, color = "Open"), linewidth = 1) +
  geom_line(aes(y = high, color = "High"), linewidth = 1) +
  geom_line(aes(y = low, color = "Low"), linewidth = 1) +
  geom_line(aes(y = close, color = "Close"), linewidth = 1) +
  geom_line(aes(y = adjusted_close, color = "adjust_close"), linewidth = 1)+
  geom_line(aes(y = change_percent, color = "change_percent"), linewidth = 1)+
  labs(title = "Price Trends Over Time", x = "Date", y = "Price") +
  scale_color_manual(values = c("Open" = "blue", "High" = "green", "Low" = "red", "Close" = "purple", "adjusted_close" = "grey", "change_percent" = "yellow")) +
  theme_minimal()

# Bar chart for Average Volume by Year
train_data$year <- year(train_data$date)
average_volume <- train_data |>
  group_by(year) |>
  summarise(avg_volume = mean(volume, na.rm = TRUE))

ggplot(average_volume, aes(x = factor(year), y = avg_volume, fill = factor(year))) +
  geom_bar(stat = "identity", color = "black") +
  scale_fill_viridis_d() +
  labs(title = "Average Volume by Year", x = "Year", y = "Average Volume") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Price Trends Over Time:
This graph visualizes the trend of Tesla stock over time, including key metrics: Open, High, Low, and Close prices and their percentage change. Each line represents one of those variables, which enables a comparative analysis of the behaviors of those variables. By far, the most relevant is the purple line that describes the Close price, representing extreme changes, spiking at certain points to reflect high market activity or volatility. The green and red lines have the daily price range, rather close to the trend within the close price. The yellow percentage change line remains rather flattish, reflecting very minor variations that underline the small day-to-day percent changes in contrast with absolute changes. This plot provides the main view on price dynamics and underlines the critical trends of a time period with increased events.

## Bar Chart for Average Volume by Year: 
This bar graph shows the average trading volume of Tesla stock by year, with each bar representing the average volume for a particular year. The increasing height over time reflects a significant rise in trading activity, especially over the last few years, reflecting growing investor interest and market activity. The biggest bars, for years 2020-2023, underline the peak of trading activity, probably supported by key events in the market, companies, and retail investor participation. The growth in this area, as reflected by rising trading volumes, is captured fairly well here.

```{r}
# Scatter plot for High vs Low Prices with Volume as color
ggplot(train_data, aes(x = high, y = low, color = volume)) +
  geom_point(alpha = 0.6) +
  scale_color_viridis_c() +
  labs(title = "High vs Low Prices with Volume Coloring", x = "High Price", y = "Low Price") +
  theme_minimal()
```

## High vs. Low Prices with Volume Coloring: 
This scatter plot displays the relationship between the high and low stock prices of Tesla, where each point represents a trading day. The points follow the line very closely, which indicates that there is a strong positive correlation between the two metrics. Points are colored according to trading volume, showing variation in market activity. Darker points are low volume, lighter points are high volume. This means that wide ranges of prices, high to low, can happen at any level of volume. The following graph helps to very clearly illustrate the stability that exists between daily price range and trading volume dynamics.

```{r}
# Boxplot for Volume by Month
train_data$month <- month(train_data$date, label = TRUE)
ggplot(train_data, aes(x = month, y = volume)) +
  geom_boxplot(fill = "lightgreen", color = "black") +
  labs(title = "Volume Distribution by Month", x = "Month", y = "Volume") +
  theme_minimal()
# Boxplot for volume by year
train_data$year <- year(train_data$date)

ggplot(train_data, aes(x = factor(year), y = volume)) +
  geom_boxplot(fill = "skyblue", color = "black") +
  labs(title = "Trading Volume Distribution by Year", x = "Year", y = "Volume") +
  theme_minimal()


```
The first box plot visualizes the trading volume distribution by month in Tesla, showing dispersion and the median of trading volumes in each month across the dataset. Though most months show very similar volume distributions, a few outliers indicate days when trading was extremely high. The second boxplot in the series is for the trading volume distribution by year. There is a definite increasing trend of volumes over the years, especially post-2020, coinciding with the major market events of Tesla. An increasing interquartile range and higher median volumes in recent years imply increased market activity and interest in Tesla's stock. Taken together, these plots both show seasonal consistency and long-term growth in trading volumes.

```{r}
# Histogram for Closing Price Distribution
ggplot(train_data, aes(x = close)) +
  geom_histogram(binwidth = 50, fill = "grey", color = "black") +
  labs(title = "Closing Price Distribution", x = "Closing Price", y = "Count") +
  theme_minimal()
```
The above histogram displays the distribution of closing stock prices of Tesla within the period of observation. Most of the closing prices fall within the range of the lower values, around 100 to 500, with the highest frequency being around 600. The distribution has a long right tail, reflecting that the number of occurrences at higher levels of price is much less frequent, with some outliers even going over 1500. This pattern suggests that the price of Tesla stock remained quite stable for most of the time within a certain range but showed occasional jumps during times of rapid growth or high volatility in the stock market. This graph skilfully highlights the skewness and overall spread of the closing prices.

```{r}
# Create a candlestick chart
chartSeries(
  xts(
    train_data[, c("open", "high", "low", "close")], 
    order.by = train_data$date
  ),
  type = "candlesticks",
  theme = chartTheme("white"),
  name = "Tesla Stock Price"
)

```
This line graph represents the trends of Tesla's stock prices from July 2010 to April 2024. The stock price showed some ups and downs at relatively low values until 2020. Beyond that, a tremendous upsurge was observed, peaking above 2000. In other words, this graph signals explosive growth in this period due to certain market activities or the publication of milestones by the firm. After the peak, the stock price experiences volatility, with notable fluctuations and a downward adjustment in the following years. The current price, as of the last data point, is 161.48, indicating a substantial correction from its peak. This visualization effectively captures Tesla's dramatic price evolution and the volatility inherent in its trading history.

```{r}
# Trend of High Price over the Years
ggplot(train_data, aes(x = year, y = high)) +
  geom_line(stat = "summary", fun = "mean", color = "red", linewidth = 1) +
  labs(title = "Trend of High Price Over the Years", x = "Year", y = "Average High Price") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
This is a line chart showing the trend in the average high prices over the years of Tesla. From 2010 to 2014, prices remain relatively stable at lower levels, reflecting an early growth stage of the company, while between 2015 and 2019, the increase is linear and steady, driven by growing market confidence. Notice the huge price increase in 2020-2021, even over 800, possibly because of events like Tesla's inclusion into the S&P 500 and an overall increased demand for electric vehicles. After 2021, the prices drop rapidly to show the correction and stabilization of the market. Overall, this chart captures Tesla's fast rise and subsequent adjustment in stock performance.

```{r}
ggplot(train_data, aes(x = date, y = volatility_10d)) +
  geom_line(color = "purple", linewidth = 1) +
  labs(title = "Volatility Over Time", x = "Date", y = "Volatility") +
  theme_minimal()

```

This line graph depicts volatility in the stock price set of Tesla over time. From 2010 until around 2019, this volatility consistently remained low and reflected relatively stable price movements. In the year 2020, there is a huge rise in it, which reflects sharp spikes in both market activity and price fluctuations. Events such as Tesla entering into the S&P 500, among broader market dynamics with respect to the pandemic, align with these observed spikes. It serves well to underline how much more volatile Tesla's stock became in its rapid growth phase and the risks and opportunities of high market interest.

* Based on the conditions assumed by the statistical learning methods, discuss their applicability to the prediction problem. 

The applicability of statistical learning methods, including logistic regression and stepwise regression, relies on the solvability of the binary classification problem of predicting Tesla stock performance. Logistic regression therefore fits well, since logistic regression models the probability given independent variables of a binary outcome. Because it considers the engineered feature using moving averages and standardized values, the assumptions of linearity in the relationship of the log-odds of the target with respect to the predictors are relatively satisfied. Similarly, given the independence of the observations concerning daily trading data, this technique assumes that the observations are non-autocorrelated.

Complementary to logistic regression, stepwise regression works on the optimization of feature selection so that only the most predictive variables are included. It iteratively compares different predictors with Akaike Information Criterion, which balances model complexity and performance. This approach proves to be very useful when handling multiple engineered features like changes in volatility and volume since it reduces redundancy and focuses only on the significant predictors.

These methods would be more appropriate to the problem due to their simplicity and interpretability. Though logistic regression works fine in instances of linear relationships, stepwise regression enhances the model's efficiency and cuts down the possibility of overfitting. The possible shortcoming with them is that they cannot depict complex nonlinear relationships that may further involve advanced models.

## Predictive analysis and results [35 points]

* Apply and document the statistical learning procedure for the predictive analysis.

```{r}
# Scale numerical variables for training and test data
numeric_vars <- c("volume", "avg_vol_20d", "change_percent", "open", "high", "low", "close", "moving_avg_5d","moving_avg_10d","volatility_10d","volume_avg_5d")
train_data_scaled <- train_data
train_data_scaled[, numeric_vars] <- scale(train_data[, numeric_vars])
test_data_scaled <- test_data
test_data_scaled[, numeric_vars] <- scale(test_data[, numeric_vars])


# Ensure the target variable is a factor
train_data_scaled$binary_target <- factor(train_data_scaled$binary_target, levels = c(0, 1), labels = c("Class0", "Class1"))
test_data_scaled$binary_target <- factor(test_data_scaled$binary_target, levels = c(0, 1), labels = c("Class0", "Class1"))

# Set up cross-validation parameters
cv_control <- trainControl(method = "cv",   # Cross-validation
                           number = 5,     # Number of folds
                           savePredictions = "final",  # Save predictions
                           classProbs = TRUE)          # Enable class probabilities

# Train logistic regression model with cross-validation
log_model_cv <- train(binary_target ~ open + high + low + close + volume + adjusted_close + avg_vol_20d + 
                        moving_avg_5d + moving_avg_10d + volatility_10d + volume_avg_5d,
                      data = train_data_scaled,
                      method = "glm",
                      family = "binomial",
                      trControl = cv_control)

# View cross-validation results
print(log_model_cv)
```
The output provides the results of a cross-validated logistic regression model using 5-fold cross-validation on a dataset of 2,771 samples with 11 predictors and two target classes (Class0 and Class1). The model achieved an average accuracy of 84.52%, indicating that it correctly classifies approximately 85% of the samples in the validation folds. The Kappa statistic is 0.6960, suggesting a substantial level of agreement between the predicted and actual classifications, accounting for chance agreement. This indicates that the model performs well in distinguishing between the two classes.
```{r}
# Load necessary library
library(MASS)

# Define the null model (intercept only) and the full model (all predictors)
null_model <- glm(binary_target ~ 1, data = train_data_scaled, family = binomial)
full_model <- glm(binary_target ~ open + high + low + close + volume + adjusted_close +
                    avg_vol_20d + moving_avg_5d + moving_avg_10d + volatility_10d + volume_avg_5d,
                  data = train_data_scaled, family = binomial)

# Perform stepwise regression
stepwise_model <- stepAIC(null_model, 
                          scope = list(lower = null_model, upper = full_model), 
                          direction = "both", 
                          trace = TRUE)

# Display the summary of the selected model
summary(stepwise_model)

```

* Estimate the performance of the statistical learning approaches on test data, using resampling methods or other measures.
```{r}
# Use the trained cross-validated model to predict test data
predicted_classes_cv <- predict(log_model_cv, newdata = test_data_scaled)
conf_matrix_cv <- confusionMatrix(predicted_classes_cv, test_data_scaled$binary_target)
print(conf_matrix_cv)
# Extract metrics
precision <- conf_matrix_cv$byClass["Pos Pred Value"]  # Precision
recall <- conf_matrix_cv$byClass["Sensitivity"]        # Recall

# Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
# Print the F1 Score
cat("F1 Score for Logistic Regression Model:", round(f1_score, 4), "\n")
```

* Evaluate the performance on the test data.
```{r}
# Plot ROC Curve for the test set
library(pROC)
predicted_probs_cv <- predict(log_model_cv, newdata = test_data_scaled, type = "prob")

roc_curve_cv <- roc(test_data_scaled$binary_target, predicted_probs_cv[, 2])
plot(roc_curve_cv, col = "blue", lwd = 2, main = "ROC Curve for Cross-Validated Model")

# Calculate AUC
auc_value_cv <- auc(roc_curve_cv)
cat("AUC for Cross-Validated Model:", auc_value_cv, "\n")
```

* Discuss the results.

The logistic regression model works in producing the results of Tesla's performance of stocks and hence providing a more interpretable and statistically valid classification framework. For the proposed model, its cross-validation accuracy was 76.59%, sensitivity was 66.86%, and specificity was 86.55%, thus showing the overall power of this model in the balance between prediction classes, as can also be obtained from the confusion matrix. An AUC score from the ROC curve of 0.88 underpins the fact that this indeed has high discriminatory power and hence, will be reliable in making out the trends of the stocks while minimizing the trade-offs between false positives and negatives.

However, logistic regression has the limitation of capturing the complex, nonlinear relationship commonly seen in financial data due to its linear nature. Supplementing with more advanced techniques such as decision trees, random forests, or other ensemble models may be able to improve predictive accuracy by capturing complex patterns and interactions among features. Nevertheless, this model of logistic regression gives robust and interpretable baselines and is really valuable for understanding the stock dynamics of Tesla and forming a basis for exploring more advanced approaches. Using cross-validation also enhances its reliability due to the insurance of good performance on a variety of data splits.

## Conclusion [15 points]

* Discuss the scope and generalizability of the predictive analysis. 

Tesla stock prediction with data illustrates poignantly how, generally speaking, the statistical learning approach is effective to resolve the binary classification problem in the estimation of stocks with the use of logistic regression. The model turned out to be generalizable with the given dataset-as was deduced from different metrics like accuracy and AUC. Coupled with feature engineering, such as moving averages, volatility, and changes in volume, the addition of depth in understanding stock behavior adds to this model's potential for generalizability of its findings to other datasets for financial markets.
However, the model has limitations. Logistic regression assumes linearity between predictors and the target variable, which may not fully capture complex, non-linear relationships inherent in financial data. Additionally, the scope of the analysis is restricted to the features and time period available in the dataset, limiting its generalizability to other stocks or markets without further validation. The model's reliance on historical data also means that it may not adapt well to abrupt changes in market conditions, such as those caused by unforeseen events.


* Discuss potential limitations and possibilities for improvement.

This can be further improved by incorporating state-of-the-art models for feature engineering, such as decision trees, random forests, or neural networks, which will leverage nonlinear relationships and interactions between features better. Further supplementation with exogenous factors economic indicators or even characteristics of sentiment can further increase higher predictive power and a wider scope of generalization out of sample. It would then test better in the out-of-sample process or cross-validated model for different periods with a higher degree of reliability as to external validity. These enhancements may make a huge difference in the performance of the model and, more importantly, its utility in real-world financial decision-making.

